---
title: "TEDA: Typicality and Eccentricity Data Analytics with Monitoring Data"
author: "David B. Ciar"
date: "25 August 2016"
output: 
  html_document: 
    fig_caption: yes
    number_sections: yes
    theme: cerulean
    toc: yes
---

```{r setup-environment, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)

# Setup the environment 
#
# =============================================================================
library(data.table)
library(lubridate)
library(dplyr)
library(stringr)
library(ggplot2)
library(reshape2)
library(bit64)
library(RcppRoll)
```


```{r setup-multiplot}

# The following multiplot code was taken from the following URI:
# http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

# Multiple plot function
#
# ggplot objects can be passed in or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot = function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots = c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout = matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx = as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

# Introduction

There have been a number of historical and contemporary analysis methods applied to data-streams rather than batch data, each with differing levels of assumptions that must be made if we are to expect the techniques to work correctly (Angelov, P. 2014).  Statistical approaches may require assumptions such as each observation being independent, random, and conforming to a previously selected distribution (iid.).  Machine learning approaches, such as K-means or KNN clustering, require some apriori knowledge of the clusters to be able to choose optimal K values, with the former also assuming clusters have a uniform circular shape.  The whole range of offline methods, where a model is trained on historical data and then used online is only viable when there can be a guarantee of no concept drift or evolution in the data-stream characteristics.  Further these offline-based models may require windows of data that would be computationally expensive to store.  A new non-parametric data analysis framework (Angelov, P. 2014) that uses distance ratios to calculate the typicality of a data point negates all of the above caveats.

This new approach, Typicality and Ecentricity Data Analytics (TEDA), was developed by Plamen Angelov at the University of Lancaster InfoLab/Data Science Institute (DSI), and first published on in 2014.  Since then there has been a number of papers on topics such as clustering (Bezerra, C.G. et al. 2016), classification and regression (Kangin, D. et al. 2015), typicality distribution functions (TDF) in place of the conventional probability distribution functions (Angelov, P. 2015), and the core TEDA process in use with example industrial machinery data as an example of detecting problematic behaviour (Costa, B.S.J. et al. 2015).  The non-parametric nature of the framework allows for deployment without configuration, while the recursive nature allows for minimum amount of necessary data to be stored between observations.

Below the batch and recursive implementations will be introduced and discussed, with examples using observation data generated from one of CEH's sensor network.  Further, the potential for use on data derived from the observations, such as the typicality of a delta, or windowed variance value will be explored, as well as the potential clustering ability on the combined QC output of the management system.

# Eccentricity and Typicality

Two papers have been published giving different implementations of the TEDA calculations.  The first of these (Angelov, P. 2014) provides three observation vectors for comparison to ensure correct implementation of the techniques.  The second provides a very different and succinct implementation of the calculations for recursive operation, used with a new clustering algorithm, and makes reference to four synthetic clustering datasets widely used in the literature.  By implementing both techniques, and comparing against the output of the reference papers, we can validate this implementation.

## Implementation

### Batch Implementation

The initial batch equations (Angelov, P. 2014) are as follows, with $\xi$ being the eccentricity parameter, and $\tau$ being the typicality parameter:

$$\pi_{k}(x_{j}) = \pi^{k}_{j} = \sum^{k}_{i=1} d_{j}, \quad k>1, \quad j>1$$

$$\xi^{k}_{j} = \frac{2\pi^{k}_{j}}{\sum^{k}_{i=1}\pi^{k}_{i}} = \frac{2\sum^{k}_{i=1}d_{j}}{\sum^{k}_{l=1}\sum^{k}_{i=1}d_{i}}$$

$$\tau = 1 - \xi$$

Both the eccentricity can be normalised as follows:

$$\zeta^{k}_{j} = \frac{\xi^{k}_{j}}{2}$$

$$t^{k}_{j} = \frac{\tau^{k}_{j}}{k-2}$$

The code implementing these equations takes the following form.  The below should work with univariate and multivariate data, where each observation is represented as a row in the vector/matrix.

```{r teda-implementation-2014-batch, echo = TRUE}

generate_batch_eccentricity = function(observation_vector){

  # Accumulated Proximity: The sum of the distances
  #   between all observation points in both directions
  sum_pi = function(val_vector){
    sum(as.matrix(dist(val_vector,diag=TRUE,upper=TRUE)))
  }
  
  # Distance between point of interest and all other points,
  #   create the matrix, then use the index to lookup the 
  #   row of interest.  `upper` is not set to true to match
  #   the paper equations - if it was set to true, the 2*
  #   operation could be removed from the eccentricity
  #   calculation
  distance_matrix = as.matrix(dist(observation_vector,
                                   diag=TRUE))

  # Generate the variables used in the calculation
  ecc = c()
  spi = sum_pi(observation_vector)
  
  # Identify this length of the observation_vector, as it
  #   may be a vector or matrix, with different accessors
  if(is.vector(observation_vector))
    obs_len = length(observation_vector)
  else
    obs_len = dim(observation_vector)[1]
  
  # Eccentricity: the relative (normalised) accumulated 
  #   proximity of that data sample as a fraction of the 
  #   accumulated proximity of all other data samples.
  #
  # Calculate eccentricity for each position
  for(curr in 1:obs_len){
    ecc = c(ecc, (2*sum(distance_matrix[curr,])) / spi)
  }
  
  return(ecc)
}

```

```{r teda-implementation-2014-batch-formatting}

plot_univariate_batch_eccentricity = function(observation_vector){
  
  # Generate the eccentricities, create a dataframe, and add
  #  the original values, typicality, and normalised values
  ecc = generate_batch_eccentricity(observation_vector)
  eccent = data.frame(ecc)
  colnames(eccent) = c("eccentricity")
  
  eccent$value = observation_vector
  eccent$typicality = 1 - eccent$ecc
  eccent$norm_eccentricity = eccent$eccentricity / 2
  eccent$norm_typicality = eccent$typicality / (length(observation_vector) - 2)
  
  # Create the threshold for normalised eccentricity/typicality
  eccent$norm_hline = 1/length(observation_vector)

  # Generate the plots, then use multiplot to display
  p1 = ggplot(eccent, 
               aes(x = value, 
                   y = eccentricity))+
        geom_bar(stat = "identity",
                 fill = "#000099",
                 position = position_dodge()) +
        ylab("Eccentricity") +
        xlab("Value") +
        ggtitle("Eccentricity")
      
  p2 = ggplot(eccent, 
               aes(x = value, 
                   y = norm_eccentricity)) +
        geom_bar(stat = "identity",
                 fill = "#000099",
                 position = position_dodge()) +
        ylab("Normalised Eccentricity") +
        xlab("Value") +
        ggtitle("Normalised Eccentricity")
  
  p3 = ggplot(eccent, 
               aes(x = value, 
                   y = typicality)) +
        geom_bar(stat = "identity",
                 fill = "#000099",
                 position = position_dodge()) +
        ylab("Typicality") +
        xlab("Value") +
        ggtitle("Typicality")
  
  p4 = ggplot(eccent, 
               aes(x = value, 
                   y = norm_typicality)) +
        geom_bar(stat = "identity",
                 fill = "#000099",
                 position = position_dodge()) +
  
        ylab("Normalised Typicality") +
        xlab("Value") +
        ggtitle("Normalised Typicality")

  multiplot(p1,p2,p3,p4, cols = 2)
}

```

Using the simple three observation vector `(20, 12, 10)` from Angelov, P. (2014), we can compare the plots below with those published.  This finds that the output is in agreement.  Do note that the typicality and normalised typicality are equivelant, as the denominator for normalisation with three observations is one.

```{r teda-implementation-2014-batch-1}
y = c(20,12,10)
plot_univariate_batch_eccentricity(y)
```

Further to the above, we add a fourth observation to the vector, `(20, 12, 10, 17)`, which changes the eccentricity compared to the above, especially for the first observation which now exceeds the $1/k$ limit.  Again the output matches the published results

```{r teda-implementation-2014-batch-2}
y = c(20,12,10,17)
plot_univariate_batch_eccentricity(y)
```

Finally, using the example of precipitation observation data recorded near Bristol in the first two weeks of January 2014, we compare the output to the original paper and find an issue.  While the overall shape of the eccentricity matches, the values do not.  This includes two observations (between 10 and 15) which are not deemed eccentric in the paper, yet are marked as such by this implementation.  

Eyeballing the observation values, they look correct, with the distances and positions looking equivelant to those in the paper.  TODO: Investigate why the eccentricity values do not equal those from the original paper.

```{r teda-implementation-2014-batch-3}
y = c(20.2,3,6.4,11.6,8.2,2.2,11.2,5.2,6.2,0.2,1,4.8,2.4,3.8)
plot_univariate_batch_eccentricity(y)
```

### Recursive Implementation

#### Initial Implementation

The recursive estimation algorithm is as follows, taken close to verbatim from Angelov. P (2014):

1. Read the next (k:=k+1) data point $x_{k}$:
2. Update:
    + $\mu^{k}$
    + $X^{k}$
3. Update/compute $\pi^{k}_{j}$
4. Update $\sum^{k}_{i=1}\pi^{k}_{i}$
5. For each observation:
    + Compute $\xi^{k}_{j}$ using the batch equation
    + Compute $\tau^{k}_{j}$  using the batch equation
    + Compute $\zeta^{k}_{j}$  using the batch equation
    + Compute $t^{k}_{j}$  using the batch equation

With the equations used in the above algorithm taking the form of:

$$\pi^{k}_{j} = k( || x_{j} - \mu^{k} ||^{2} + X^{k} - || \mu^{k} ||^{2})$$

$$\mu^{k} = \frac{k-1}{k} \mu^{k-1} + \frac{1}{k} x_{k}, \quad \mu^{1} = x_{1}$$

$$X^{k} = \frac{k-1}{k} X^{k-1} + \frac{1}{k} || x_{k} ||^{2}, \quad X^{1} = ||x_{1}||^{2}$$

$$\sum^{k}_{i=q} \pi^{k}_{i} = \sum^{k-1}_{i=1}\pi^{k-1}_{i} + 2\pi^{k}_{k}, \quad \pi^{1}_{1} = 0$$


It should be noted that we do not expect the exactly same output as the batch method due to a number of reasons.  First is that we will lose the first two observations (k = 1:2), as until k = 3 the eccentricity values will be meaningless.  Second is that the recursive values at each step are not the same as those used in the batch method, though as more data is processed they become closer approximations, and so this is another reason for a different outcome.  Third, the threshold of $1/k$ will be different for every observation, with the value approaching zero as the dataset continues to grow.

Of note is that I am unsure of how the reference paper has used the recursive estimation, yet still given an eccentricity value to the $20.2$ initial observation vector value.  If we did not remove the first two eccentricity observations, then the value of $20.2$ would have eccentricity of zero, while the following value would have eccentricity of one.  This is in conflict with the graphs provided in the reference paper that show the full fourteen value vector.

The first plots below show the eccentricity and typicality using the same layout as the batch plots, with the x-axis ordered by the observation value.  The plots are not exactly the same, however a similar shape does emerge.  The two largest observation values have typicality values larger than in the batch method, where in this implementation they are regarded as typical.  The code is as follows:

```{r teda-implementation-2014-recursive, echo=TRUE}

# Recursive method takes a number of values (k, mu, X, sum_pi)
#  which need to be past each time, as they record the current
#  state of the algorithm.  Univariate.
generate_recursive_eccentricity = function(curr_obs, k, mu, X, sum_pi){
  
  # On the first iteration, there is not enough data to begin
  #  to return eccentricities.
  if(k == 1){
    k_mu = curr_obs
    k_X = curr_obs^2
    k_pi = 0
    k_ecc = 0
    k_sum_pi = 0
    
    return(c(k_mu, k_X, k_sum_pi, k_ecc))
  }else{
    # On the second and above iterations, the calculated values
    #  are returned, though, the eccentricity does not begin to
    #  make sense until k >= 3.
    
    # The following four variables: k_mu, k_X, k_pi, and k_sum_pi
    #  use the recursive definitions
    k_mu = (((k - 1) / k) * mu) + ((1 / k) * curr_obs)
    
    k_X = (((k - 1) / k) * X) + ((1 / k) * (curr_obs^2))
    
    k_pi = k * ((curr_obs - k_mu^2) + k_X - (k_mu^2))
    
    k_sum_pi = sum_pi + (2 * k_pi)
    
    # This is the eccentricity definition from the batch algorithm
    k_ecc =  (2*k_pi / k_sum_pi)
    
    return(c(k_mu, k_X, k_sum_pi, k_ecc))
  }
}
```

Due to the changing $1/k$ threshold per observation and the x-axis being ordered by value rather than processing order, the threshold line would be very erratic, and so the fill-colour yellow has been used to denote those values that are outwith the threshold.  While not a complete match to the batch version, it is a close approximation.

```{r teda-implementation-2014-recursive-formatting}

# Function to manage the eccentricity calculation with 
#  the state variables.
# - reorder: plot values in natural order or index/processing
#            order
# - all_data: plot all data, or remove the first two 
#            eccentricity values
# - model: whether to use the 2014 or 2016 algorithm
# - m: the threshold to use with the 2016 algorithm
plot_univariate_recursive_eccentricity = function(observations, 
                                                  reorder,
                                                  all_data, 
                                                  model = 2014, 
                                                  m = 2){
   
  # Holder for the eccentricity
  ecc = c()
  
  # Choose which model to use - the 2016 model is defined after this block, 
  #   but not before it is used, making this OK.  Potentially pass the model
  #   as an argument if this is bad form.
  if(model == 2014){
    # Holder for the state values
    mu = NA
    X = NA
    sum_pi = NA
    
    # For each observation in the dataset
    for(k in 1:length(observations)){
      curr_vals = generate_recursive_eccentricity(observations[k], k, mu, X, sum_pi)
      mu = curr_vals[1]
      X = curr_vals[2]
      sum_pi = curr_vals[3]
      ecc = c(ecc,curr_vals[4])
    }
  }else{
    # Holder for the state values
    previous_mean = observations[1]
    previous_var = 0

    # For each observation in the dataset
    for(k in 1:length(observations)){
      curr_vals = generate_recursive_eccentricity_two(observations[k], previous_mean, previous_var, k)
      previous_mean = curr_vals[1]
      previous_var = curr_vals[2]
      ecc = c(ecc,curr_vals[3])
    }
  }
  
  # Create the dataframe with eccentricity, original values,
  #  other derived information
  eccent = data.frame(ecc)
  colnames(eccent) = c("eccentricity")
  eccent$value = observations
  eccent$idx = 1:dim(eccent)[1]
  eccent$norm_eccentricity = eccent$eccentricity / 2
  eccent$typicality = 1 - eccent$eccentricity 
  eccent$norm_typicality = eccent$typicality / (dim(eccent)[1] - 2)
  
  # Unlike the batch approach, the 1/k threshold changes with 
  #  every observation, and so it is calculated as a vector,
  #  with the technique dependent on the model used
  if(model == 2014){
    eccent$threshold = 1/1:dim(eccent)[1]
    eccent$threshold_over = eccent$norm_eccentricity > eccent$threshold 
    eccent$threshold_under = eccent$norm_typicality < eccent$threshold    
  }else{
    eccent$threshold = ((m^2) + 1) / (2 * 1:dim(eccent)[1])
    eccent$threshold_over = eccent$norm_eccentricity > eccent$threshold
    eccent$threshold_under = eccent$norm_typicality < eccent$threshold
  }

  # If the data is to be plotted in the same order as 
  #  observations were processed, create a factor of the 
  #  observation value
  if(!reorder)
    eccent$value = factor(eccent$value, levels=eccent$value)
  
  # If we are not using all the data, remove the first two 
  #  observations who's eccentricity values (0 and 1) do not
  #  make sense.
  if(!all_data)
    eccent = eccent[3:dim(eccent)[1],]
  
  # Generate the individual plots, then display using multiplot
  p1 = ggplot(eccent)+
    geom_bar(aes(x = value, 
                 y = eccentricity,
                 group = idx),
             stat = "identity",
             fill = "#000099",
             position = "dodge") +
    ylab("Eccentricity") +
    xlab("Value") +
    ggtitle("Eccentricity")
  
  p2 = ggplot(eccent)+
    geom_bar(aes(x = value, 
                 y = norm_eccentricity, 
                 fill=threshold_over,
                 group = idx),
             stat = "identity",
             position = position_dodge(),
             width = 0.5) +
    ylab("Normalised Eccentricity") +
    xlab("Value") +
    ggtitle("Normalised Eccentricity") +
    scale_fill_manual(values=c("#000099",'#ffff66')) +
    guides(fill=FALSE)
    
  p3 = ggplot(eccent)+
    geom_bar(aes(x = value, 
                 y = typicality,
                 group = idx),
             stat = "identity",
             fill = "#000099",
             position = position_dodge()) +
    ylab("Typicality") +
    xlab("Value") +
    ggtitle("Typicality")
  
  p4 = ggplot(eccent)+
    geom_bar(aes(x = value, 
                 y = norm_typicality, 
                 fill=threshold_over,
                 group = idx),
             stat = "identity",
             position = position_dodge(),
             width = 0.5) +
    ylab("Normalised Typicality") +
    xlab("Value") +
    ggtitle("Normalised Typicality") +
    scale_fill_manual(values=c("#000099",'#ffff66')) +
    guides(fill=FALSE)
  
  multiplot(p1,p2,p3,p4, cols = 2)

}

# Dataset
y = c(20.2,3,6.4,11.6,8.2,2.2,11.2,5.2,6.2,0.2,1,4.8,2.4,3.8)
plot_univariate_recursive_eccentricity(y,TRUE,FALSE,2014)

```

#### Clustering Based Implementation

Finally, the most recent recursive method (Bezerra, C.G. 2016) is implemented, which uses a much simpler set of three equations as follows:

$$\xi(x_{k}) = \frac{1}{k}+\frac{(\mu_{k}-x_{k})^{T}(\mu_{k}-x_{k})}{k\sigma^{2}_{k}}$$

$$\mu_{k} = \frac{k-1}{k} \mu_{k-1} + \frac{1}{k}x_{k}, \quad \mu_{1} = x_{1}$$

$$\sigma^{2}_{k} = \frac{k-1}{k} \sigma^{2}_{k-1} + \frac{1}{k-1} || x_{k} - \mu_{k}||^{2}, \quad \sigma^{2}_{1} = 0$$

The normalised eccentricity, typicality, and normalised typicality are calculated in the same way as before.  The authors put forward an equation for outlier detection, using the Chebyshev inequality, where $m$ is the number of standard deviations away from the data we expect outliers to appear:

$$ \zeta(x_{k}) > \frac{m^{2} + 1}{2k}, \quad m > 0$$

```{r teda-implementation-2016-recursive, echo = TRUE}

# Recursive method takes the state variables of previous mean 
#  and variance, and the current timestep position.  Univariate.
generate_recursive_eccentricity_two = function(curr_val, previous_mean, previous_var, k){

  # Calculate the recursive mean value
  rec_mean = function(k, previous_mean, curr_val){

    (((k - 1)  / k) * previous_mean) + ((1 / k) * curr_val)
  }

  # Calculate the recursive variance value
  rec_var = function(k,prev_var, curr_val, curr_mean){

    (((k - 1) / k) * prev_var) + (1 / (k - 1)) * ((curr_val - curr_mean)^2)
  }

  # Calculate the recursive eccentricity value
  rec_ecc = function(k, curr_mean, curr_var){

    (1 / k) +  (((curr_mean - curr_val)^2) / (k * curr_var))

  }

  # If the initial timestep, set the initial parameter values,
  #  else, use the previous timestep values
  if(k == 1){
    mean_val = curr_val
    var_val = 0
    ecc_val = rec_ecc(k, mean_val, var_val)
  }else {
    mean_val = rec_mean(k, previous_mean, curr_val)
    var_val = rec_var(k, previous_var, curr_val, mean_val)
    ecc_val = rec_ecc(k, mean_val, var_val)
  }
  return(c(mean_val,var_val,ecc_val))
}

```

Using the vector from the 2014 paper representing rainfall as in the previous recursive implementation, with $m$ set to $1$, we get the following output.  We can see that this matches the previous implementation, with the same observations identified as when using the $1/k$ threshold.

```{r teda-implementation-2016-recursive-1}
plot_univariate_recursive_eccentricity(y,TRUE,FALSE,2016,1)
```

When $m$ is set to $2$ however, we can see that none of the observations were eccentric enough to be identified as outliers.

```{r teda-implementation-2016-recursive-2}
plot_univariate_recursive_eccentricity(y,TRUE,FALSE,2016,2)
```

## Evaluation Using Data with No Quality Labels

A number of different observation types will be used in evaluating the behaviour of the recursive TEDA algorithm, including the raw observations, the observation deltas, and the windowed variance.  The data source of the observations is the Ecological Change Network (ECN) meteorology data (Rennie, S. et al. 2015).  The data contains observations from 1992 to 2012, however the subset of observations from 1998 to 2000 will be used initially.

### Ecological Change Network Meteorological Data

```{r teda-evaluation-ecn-load-data, cache = TRUE}

# Load the data, downloadable from the CEH data catalogue:
# https://catalogue.ceh.ac.uk/documents/e1d33b37-f1d4-4234-a0d5-8bf4e657f653

ecn_data = fread('~/Data/ECN_SWE_Data/Data/T04_MoorHouse_UpperTeesdale.csv')
colnames(ecn_data) = tolower(colnames(ecn_data))

# Within the data are the following unique fieldnames representing
#  the individual sensors at the recording site:
#  1:      WDIR
#  2:    WETTMP
#  3:    WSPEED
#  4:    DRYTMP
#  5:    NETRAD
#  6:      RAIN
#  7:     SOLAR
#  8:    STMP10
#  9:    STMP30
# 10:    SURWET
# 11:    SWATER
# 12:    ALBGRD
# 13:    ALBSKY
# 14:   DRYTMP_
# 15:        RH

# Format the date time fields correctly
ecn_data$timestamp = ymd_hms(ecn_data$sdate)
hour(ecn_data$timestamp) = ecn_data$shour
ecn_data$year = year(ecn_data$timestamp)
ecn_data$doy = yday(ecn_data$timestamp)
  
```

```{r teda-evaluation-ecn-generate-outliers}

# Function to generate the eccentricity values,
#  create the dataframe complete with eccentricity
#  and outlier status, and return.

# - observations: dataframe with a 'value' column
#                 with observation values

evaluate_teda_vect = function(observations){

  # Holder for the eccentricity
  ecc = c()

  # Holders for the state values
  previous_mean = observations$value[1]
  previous_var = 0
  
  # For each observation in the dataset, generate
  #  the eccentricity value
  for(k in 1:length(observations$value)){
    curr_vals = generate_recursive_eccentricity_two(observations$value[k], 
                                                     previous_mean, 
                                                     previous_var, k)
    previous_mean = curr_vals[1]
    previous_var = curr_vals[2]
    ecc = c(ecc,curr_vals[3])
  }

  # Augment the return dataset with the derived values  
  observations$eccentricity = ecc
  observations$norm_eccentricity = ecc / 2
  observations$threshold = 5 / (2 * 1:dim(observations)[1])
  observations$outlier = observations$norm_eccentricity > observations$threshold

  return(observations)
}


# Generate the deltas, retrieve the eccentricity, then return as
#  the augmented dataframe
generate_delta_teda_vect = function(observations){
  
  observation_deltas = observations$value[1:(dim(observations)[1]-1)] - 
                          observations$value[2:dim(observations)[1]]
  
  time_deltas = observations$timestamp[1:(dim(observations)[1]-1)] - 
                          observations$timestamp[2:dim(observations)[1]]

  # Augment the original data frame
  observations$delta = c(0, observation_deltas)
  observations$time_delta = c(0, time_deltas)

  # All data within this dataset should have a 1 hour spacing, if this
  #  is exceeded, then the delta will also be unrealistic. Filter 
  #  based on this
  observation_deltas = observation_deltas[time_deltas == -1]

  # Generate the eccentricities
  observation_deltas = data.frame(observation_deltas)
  colnames(observation_deltas) = c("value")
  observation_deltas = evaluate_teda_vect(observation_deltas)
  
  # Augment the original data frame and return
  observations$eccentricity = NA
  observations$eccentricity[c(0,time_deltas) == -1] = observation_deltas$eccentricity
  
  observations$norm_eccentricity = NA
  observations$norm_eccentricity[c(0,time_deltas) == -1] = observation_deltas$norm_eccentricity
  
  observations$threshold = NA
  observations$threshold[c(0,time_deltas) == -1] = observation_deltas$threshold
  
  observations$outlier = NA
  observations$outlier[c(0,time_deltas) == -1] = observation_deltas$outlier
  
  return(observations)
}

generate_sigma_teda_vect = function(observations,window_size, tumbling = FALSE){

  by_obs = 1
  
  if (tumbling == TRUE)
    by_obs = (window_size - 1)
  
  var_vals = roll_var(x = observations$value,
                    n = window_size,
                    by = by_obs,
                    align = "center",
                    na.rm = TRUE)
  
  edge_buffer = (window_size - 1) / 2
  #observations = observations[(1 + edge_buffer):(dim(observations)[1] - edge_buffer),]
  observations$sigma = NA
  observations$sigma[(1+edge_buffer):(dim(observations)[1] - edge_buffer)] = var_vals

  # Generate the eccentricities
  observation_var_vals = data.frame(var_vals)
  colnames(observation_var_vals) = c("value")
  observation_var_vals = evaluate_teda_vect(observation_var_vals)
  
  # Augment the original data frame and return
  observations$eccentricity = NA
  observations$eccentricity[(1 + edge_buffer):(dim(observations)[1] - edge_buffer)] =
    observation_var_vals$eccentricity
  
  observations$norm_eccentricity = NA
  observations$norm_eccentricity[(1 + edge_buffer):(dim(observations)[1] - edge_buffer)] =
    observation_var_vals$norm_eccentricity
  
  observations$threshold = NA
  observations$threshold[(1 + edge_buffer):(dim(observations)[1] - edge_buffer)] =
    observation_var_vals$threshold
  
  observations$outlier = NA
  observations$outlier[(1 + edge_buffer):(dim(observations)[1] - edge_buffer)] =
    observation_var_vals$outlier  
  
  return(observations)
}

```

#### Dry Bulb Temperature - Observations

The dry bulb temperature is recorded using a sensor protected from the elements, but which is in contact with the air (such as a sensor located within a Stevenson screen).  The data has been subset to the 1998 - 2000 range.  We can see that the daily changes are quite varied, and that there is an increase in the average temperature over the summer.  

```{r teda-evaluation-ecn-drybulb-data, cache = TRUE}

# Subset the ECN observations and retrieve the dry temperature
#  only.
dry_temp = ecn_data %>%
  filter(fieldname == "DRYTMP" &
           year >= 1998 &
           year <= 2000 &
           !is.na(value))

```




```{r evaluation-ecn-drybulb-introduction}
ggplot(dry_temp) +
  geom_point(aes(x=doy, y=value),colour="#000099") +
  facet_grid(year ~ .) +
  ylab("Temperature / degrees Celsius") +
  xlab("Day of year") +
  ggtitle("Dry Bulb Temperature 1998 - 2000")

```

Running the eccentricity analysis we get the following, where the outliers have been highlighted in the data.  The threshold used was $m = 2$, and this appears to have generated an area where values lower than minus five, or higher than fifteen are marked as outliers.

```{r evaluation-ecn-drybulb-teda-1}

dry_temp = evaluate_teda_vect(dry_temp)


ggplot(dry_temp) +
  geom_point(aes(x=doy, y=value,
                 colour = outlier)) +
  facet_grid(year ~ .)+
  ylab("Temperature / degrees Celcius") +
  xlab("Date Time") +
  ggtitle("Temperature with Outliers Highlighted") +
  scale_colour_manual(name = "Outlier?",
                        values = c("#000099",'#ffff66'))    

```

Zooming in on the first group of observations marked as outliers, we can see that in the month of May, there were seven, two, and five days where the temperatures exceeded the eccentricity limit.

```{r evaluation-ecn-drybulb-teda-2}
dry_temp_sub = dry_temp %>%
  filter(doy >= 125 &
           doy <= 150)
year(dry_temp_sub$timestamp) = 0

ggplot(dry_temp_sub) +
  geom_point(aes(x=timestamp, y=value,
                 colour = outlier)) +
  facet_grid(year ~ .) +
  ylab("Temperature / degrees Celcius") +
  xlab("Date Time") +
  ggtitle("Temperature with outliers highlighted (subset)") +
  scale_colour_manual(name = "Outlier?",
                      values = c("#000099",'#ffff66'))    
```

Plotting the density of the observation values, faceted on whether the observation was marked as an outlier or not, we can clearly see that acceptable observation values are centred around five, with plus or minus ten either way.  There are however observations around this mid-point that are getting marked as outliers.  On closer inspection these are all observations at the start of 1998 as the algorithm is starting, as opposed to say cold observations in summer, or warm observations in winter.

```{r evaluation-ecn-drybulb-teda-3}
dry_temp$outliers = "Eccentricity > 2 sigma"
dry_temp$outliers[dry_temp$outlier == FALSE] = "Eccentricity <= 2 sigma"

ggplot(dry_temp, aes(x=value)) +
  geom_histogram(aes(y=..density..),
            binwidth = 0.5,
            colour = "black",
            fill = "#ffff66") +
  geom_density(alpha=.2, fill = "#000099")+
  facet_grid(. ~ outliers) +
  ylab("Density") +
  xlab("Temperature / degrees Celsius") +
  ggtitle("Density of Observations, Faceted by Eccentricity Threshold")

```

#### Dry Bulb Temperature - Delta

The delta is the rate of acceleration between an observation and the subsequent observation following it.  It would be expected that the delta values increase in the summer, and decrease over winter, and we can see that the largest variations are within the summer months below.  This plot shows that most deltas fall between plus or minus one, centered around zero.  The observations in all of the analysis below have had the delta of any observations that do not fall within one hour of each other removed.

```{r evaluation-ecn-drybulb-delta-intro}

  delta_observations = generate_delta_teda_vect(dry_temp)

  # remove observations where the time delta is not as expected
  delta_observations = delta_observations %>%
    filter(time_delta == -1)
  
  ggplot(delta_observations) +
   geom_point(aes(x=doy,y=delta),colour="#000099") +
    facet_grid(year ~ .) +
  ylab("Change in Temperature / degrees Celsius") +
  xlab("Day of Year") +
  ggtitle("Temperature Delta of Consecutive Observations")

```

Running the delta values through the eccentricity algorithm we get the following output, where the core of the values are OK, but those exceeding plus or minus 1.5 on either side of zero are being marked as outliers.  It is also very apparent that the summer period has by far the greater amount of outliers than the start and end of each year.

```{r evaluation-ecn-drybulb-delta-eccentricity}

ggplot(delta_observations) +
  geom_point(aes(x=doy,y=delta, colour = outlier)) +
  facet_grid(year ~ .)+
  ylab("Change in Temperature / degrees Celsius") +
  xlab("Date Time") +
  ggtitle("Temperature Delta with Outliers Highlighted") +
  scale_colour_manual(name = "Outlier?",
                      values = c("#000099",'#ffff66'))    

```

Looking at a subset of the data, where we can view the individual hourly observations, it is clear that the areas where the outliers occur are those that are sparcely filled due to rapid changes of value.  Again, this makes sense as these rapid changes occur more often in the summer months, leading to a greater number of outliers.

```{r evaluation-ecn-drybulb-delta-eccentricity-subset}

delta_observations_sub = delta_observations %>%
  filter(doy >= 100 &
           doy <= 150)

year(delta_observations_sub$timestamp) = 0

ggplot(delta_observations_sub) +
  geom_point(aes(x=timestamp, y=value,
                 colour = outlier)) +
  facet_grid(year ~ .) +
  ylab("Change in Temperature / degrees Celcius") +
  xlab("Date Time") +
  ggtitle("Temperature Delta with Outliers Highlighted (subset)") +
  scale_colour_manual(name = "Outlier?",
                      values = c("#000099",'#ffff66'))    
```

Looking at the density plot we can see that the observation values associated with the deltas that exceed the eccentricity form a distribution with a mean value greater than those whos deltas do not exceed the eccentricity.

```{r evaluation-ecn-drybulb-delta-eccentricity-distribution}

delta_observations$outliers = "Eccentricity > 2 sigma"
delta_observations$outliers[delta_observations$outlier == FALSE] = "Eccentricity <= 2 sigma"
ggplot(delta_observations, aes(x=value)) +
  geom_histogram(aes(y=..density..),
            binwidth = 0.5,
            colour = "black",
            fill = "#ffff66") +
  geom_density(alpha=.2, fill = "#000099")+
  facet_grid(. ~ outliers) +
  ylab("Density") +
  xlab("Temperature / degrees Celsius") +
  ggtitle("Density of Observations, Faceted by Delta Eccentricity Threshold")

```

#### Dry Bulb Temperature - Sigma (variance)

The plot below shows variance, calculated over a seven hour sliding (rather than tumbling) window, without taking into account potential gaps in the time-sequence (of which some do exist).  There are clear spikes in the winter months where the temperature can drop overnight, and in the spring-summer months where the temperature can rise high during the day.

```{r evaluation-ecn-drybulb-sigma}

# Use a sliding, rather than tumbling windod (i.e. no 'by' parameter)

  sigma_observations = generate_sigma_teda_vect(dry_temp, 7)

  ggplot(sigma_observations) +
   geom_point(aes(x=doy,y=sigma),colour="#000099") +
    facet_grid(year ~ .) +
  ylab("Variance") +
  xlab("Day of Year") +
  ggtitle("Temperature Sigma of 7 Consecutive Observations")
  
```

Using the same data as above, but this time highlighting the eccentric values we get the following output:

```{r evaluation-ecn-drybulb-sigma-2}
ggplot(sigma_observations) +
  geom_point(aes(x=doy,y=sigma, colour = outlier)) +
  facet_grid(year ~ .)+
  ylab("Variance") +
  xlab("Date Time") +
  ggtitle("Temperature Sigma of 7 Consecutive Observations") +
  scale_colour_manual(name = "Outlier?",
                      values = c("#000099",'#ffff66'))    

```

Similarly, with a larger window size of 13 consecutive observations, we see that the normal distance range is greater due to the window size being larger, and so fewer observations are marked as being eccentric.

```{r evaluation-ecn-drybulb-sigma-22}
  sigma_observations = generate_sigma_teda_vect(dry_temp, 13)

ggplot(sigma_observations) +
  geom_point(aes(x=doy,y=sigma, colour = outlier)) +
  facet_grid(year ~ .)+
  ylab("Variance") +
  xlab("Date Time") +
  ggtitle("Temperature Sigma of 13 Consecutive Observations") +
  scale_colour_manual(name = "Outlier?",
                      values = c("#000099",'#ffff66'))    

```

## Findings

TODO: Need to generate test cases, identify existing data that has known anomalies.  Possibly look into the "ADAM" method paper which ran two models concurrently, one that included outliers in the recursive computation, one that did not, until the outliers became the new normal.

# TEDA Clustering 


# References

Angelov, P., 2014. Outside the box: an alternative data analytics framework. Journal of Automation, Mobile Robotics & Intelligent Systems 8, 29–35. doi:10.14313/JAMRIS_2-2014/16

Angelov, P., 2015. Typicality distribution function - A new density-based data analytics tool. IEEE, pp. 1–8. doi:10.1109/IJCNN.2015.7280438

Bezerra, C.G., Costa, B.S.J., Guedes, L.A., Angelov, P.P., 2016. A new evolving clustering algorithm for online data streams. IEEE, pp. 162–168. doi:10.1109/EAIS.2016.7502508

Costa, B.S.J., Bezerra, C.G., Guedes, L.A., Angelov, P.P., 2015. Online fault detection based on Typicality and Eccentricity Data Analytics. IEEE, pp. 1–6. doi:10.1109/IJCNN.2015.7280712

Kangin, D., Angelov, P., 2015. Evolving clustering, classification and regression with TEDA. IEEE, pp. 1–8. doi:10.1109/IJCNN.2015.7280528

Rennie, S., Adamson, J., Anderson, R., Andrews, C., Bater, J., Bayfield, N., Beaton, K., Beaumont, D., Benham, S., Bowmaker, V., Britt, C., Brooker, R., Brooks, D., Brunt, J., Common, G., Cooper, R., Corbett, S., Critchley, N., Dennis, P., Dick, J., Dodd, B., Dodd, N., Donovan, N., Easter, J., Eaton, E., Flexen, M., Gardiner, A., Hamilton, D., Hargreaves, P., Hatton-Ellis, M., Howe, M., Kahl, J., Lane, M., Langan, S., Lloyd, D., McElarney, Y., McKenna, C., McMillan, S., Milne, F., Milne, L., Morecroft, M., Murphy, M., Nelson, A., Nicholson, H., Pallett, D., Parry, D., Pearce, I., Pozsgai, G., Rose, R., Schafer, S., Scott, T., Sherrin, L., Shortall, C., Smith, R., Smith, P., Tait, R., Taylor, C., Taylor, M., Thurlow, M., Turner, A., Tyson, K., Watson, H., Whittaker, M., Wilkinson, M. (2015). UK Environmental Change Network (ECN) meteorology data: 1992-2012. NERC Environmental Information Data Centre. http://doi.org/10.5285/e1d33b37-f1d4-4234-a0d5-8bf4e657f653
