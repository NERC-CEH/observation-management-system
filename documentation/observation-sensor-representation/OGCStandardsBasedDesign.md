# OGC Based Observation and Sensor Representation

## Introduction

The design of the system has been with the aim to both represent the data in the best way for management, while at the same time making it relatively simple to modify the data in a higher layer to support the SOS and O&M standards (where we plan to incorporate the TimeSeries extension to O&M within WaterML2).  The meta-data the O&M standard requires is generally the type of data that would be stored regardless, and so it is more the design of the data store around these fields that has to be decided.  The design will not mirror exactly the format of the standard, as it will not always make sense to store the full representation for every observation.  There is also the design choice whereby the database will only hold the minimum necessary information to serve the observations, while the majority of the meta-data will be stored and queried within a catalogue representation of either the sensor/procedure/domain coverage data.

As in the rest of the documentation, the examples will be based around data generated within the Lake Ecology Group, however the observations are represented in a generalised way making no boundaries to data from other domains.  The Lake Ecology Group data is generated from automated sensors, manual sampling, derived model data, and forecasts.  Following the terminology of [WaterML2.0 Timeseries](http://www.opengeospatial.org/standards/waterml) these can be defined as: in-situ fixed observation style, in-situ manual observations, and complex data products.

## SWE Observation Centric Design

Using the modelling approach found in document ["D2.9 Draft Guidelines for the use of Observations & Measurements and Sensor Web Enablement-related standards in INSPIRE Annex II and III data specification development"](http://inspire.ec.europa.eu/documents/Data_Specifications/D2.9_O&M_Guidelines_v2.0rc3.pdf), diagrams have been created to show the modelling of sensor, sample, forecast, and QC observation data, with point based, specimen based, and curve based features of interest.

### Sensor Observations

Sensor observations are generated by hardware attached to a Lake Observation Platform, which is stationary at a point on the lake.  Sensors can be found at different locations in reference to the platform, whether above to sense phenomena such as atmospheric temperature, wind speed, and relative humidity, or below for phenomena such as lake temperature, turbidity, and dissolved oxygen.

The location of the platform is modelled as a feature of interest, specifically a `sf_samplingpoint`, described by its latitude and longitude.  The specific height or depth (z axis) of individual sensors are recorded as a parameter within the sensor definition for those that are stationary on the platform.  It is also possible to state the x and y axis positions relative to the sampling point for each individual sensor.  For sensors that are not stationary, for example sensors that can move vertically in the water column, the depth of the sensor relative to the feature is stored as a parameter within the individual observation.  The sampled feature of the platform sampling point is the lake that the platform is located on, while the domain feature at this point is undecided, but could also be the lake, or the Cumbrian Lakes, or a longer feature hierarchy.  In WaterML2 the lake would be the domain feature while the platform would be the monitoring point of the domain feature.

The sensor data is such that the values recorded vary based on time, leading to the classification of the observations as a type of discrete coverage.  This type of observation has been extended in the WaterML2 TimeSeries specification, whereby rather than defining a time-series as simply a collection of measurement observations, the coverage type is extended to provide the spatio-temporal context of the time-series rather than only an OM Measurement.  Of the two types of coverage, domain or time-value-pairs (TVP), the TVP representation seems the better of the two for serving to users, however in the storage representation a combination of both will be used as appropriate to the particular piece of information being recorded.  

By keeping the above in mind, extra meta-data such as the intendedObservationSpacing, interpolationType, and temporal extent can be recorded for the time-series, with the extent being updated on each observation addition.  This data can be modelled in the domain way rather than per-observation in the storage representation, as could other meta-data such as the aggregation duration, process type, and process reference as these are unlikely to change often if at all.  The distinction between observed property and medium can also be at this higher level, however the distinction between observed property and medium needs to be coordinated with the design/use of the phenomena vocabulary.

Regarding the state and quality of the observation, there are explicit meta-data to deal with this such as: processing code, status, data quality code, qualifier, and related observation fields.  The combination of these fields allow the description of the processing state (provisional or validated for instance) and the overall quality code (good, suspect, estimate, poor, unchecked, missing), supplemented by the qualifier entry which allows for greater detail into how the quality code was derived.

For a basic graphical representation of a sensor within the system, the below shows the feature of interest being the buoy on the lake, with the lake as the domain feature.  The procedure generating the observations is the PRT sensor, the observed phenomena is temperature (while the sampling medium would be water).  The values have only a single datetime entry and a value, where the datetime entry is the phenomenon time.

![Sensor Observation Model](graphics/SensorObservation.png?raw=true "Sensor Observation")

### Model Observations

When describing model output, the main difference between a physical sensor and an abstract process is the meta-data associated with the definition of the process compared to the sensor.  Regarding the observations generated there is little difference, with only the addition of result time being added.  The result time denotes when the model output was generated, while the phenomenon time denotes when the forecast observations were said to have been observed.  Model output can also have a valid time period for which the forecast is valid/useful etc.

In the example diagram below, the difference between the phenomenon and result times are shown, with the result time occurring two minutes after the phenomenon time to indicate the delay in processing the data.  The procedure creating the observation is linked to the PRT sensors to show that it is based on their joint observation outputs.  The feature in this example is also modified from the sensor example as it shows the water profile as the feature, which is of type `sf_samplingcurve` rather than `sf_samplingpoint`.  This is because it models the temperature profile within the water column and necessarily has a z-axis defining the depth range of the observed feature.

![Sensor Observation Model](graphics/ProfileObservation.png?raw=true "Model Profile Observation")

### Sample/Specimen Observations

Observations generated from a sample or specimen are similar in nature to both sensor and model observations, with the exception being that it is the specimen that is the feature of interest.  The results may or may not have a result time associated with them, though in this implementation that will be the case.  The phenomenon time is explicit in the sample's collection meta-data.

![Sensor Observation Model](graphics/SampleObservation.png?raw=true "Sample Observation")

### Forecast Observations

Forecast observations are the most difficult type in terms of modelling the observation centric format, due to a number of questions such as:

* How does one model a forecasting process that can forecast multiple phenomena from multiple sensors?
* How does one model a forecasting process that by its nature is forecasting the sensor output rather than the observed phenomena?  For example, if there are two temperature sensors at the same depth in a lake separated by a few centimetres along the X and Y axis, located at the same feature of interest (the buoy), but the forecasts for both may be/would likely be different based on differing inputs and sensor calibrations/drift.
* Taking the above two points, how does one model the dataflow such that a user can easily tell which physical sensor or model observations were used as input to the forecasting process?

While the O&M standard and accompanying INSPIRE guidelines do not explicitly disallow the use of a recording component (such as a specific temperature sensor) as a feature of interest, they are clear that they do not believe this to be a good way to model around an observation.  The below diagram is an example of this discouraged way of modelling.  It shows the process 'Forecast Diurnal Signals', a forecasting process that can work with any sensed phenomena that has a strong diurnal component to its signal.  If we were to separately put the temperature sensors (mentioned in the bullet points above) through the process we would generate forecast observations, however unless (as is displayed) the process who's observations are being forecast is used as the feature of interest, how would it be visible from the observation model which forecast was associated with which observation.

![Sensor Observation Model](graphics/ForecastObservationSensorFOI.png?raw=true "Forecast Observation Sensor FOI")

The above design is not satisfactory for a number of reasons:

* that it goes against the spirit of the standard and guidelines
* that for features of interest with multiple sensors in similar positions recording the same phenomena it is necessary to use the sensor as the feature of interest, but if there was only one sensor for each phenomena this would not be an issue
* data flow is implicit, in that due to the feature of interest being the sensor, it implies the input to the forecasting function is from that sensor however this is not necessarily the case (multiple different sensor observations may be used to forecast a single sensor)

Rather than keeping the forecasting process as a single instance within the system, it seems the best way to both explicitly represent data flow and the observations being forecast, is to use multiple instances inherited from a base instance.  For example, with the two temperature sensors, if one were called Sonde1TEMP, the other Sonde2TEMP, then instances of the forecast process called Sonde1TEMP/FORECAST/MethodSDS and Sonde2TEMP/FORECAST/MethodSDS would be created.  The inputs to these instances would be the outputs of the Sonde1TEMP and Sonde2TEMP, representing the dataflow, while the feature of interest would be the same lake platform as the sensors, with the observed property also being the same as the sensors.  The only two types of question this design appears to raise are: 

* when modelling as a SensorML document rather than using an ontology like SSN, how is the relationship between the sensor and the forecast instance described, are the forecast instance and sensor both components of a greater logical sensor grouping
* where do parameters such as depth get stored for the forecast output: as observation parameters, inferred from the sensor, or within the process?

For the first point, when using SensorML, the inputs and outputs between different sensors and processes are modelled by subsuming lower-level components into higher level components and stating the connections between them.  For instance a logical Sonde1 SensorML definition may state that the Sonde1TEMP physical component is a component of Sonde1, while the Sonde1TEMP/FORECAST/MethodSDS is also a component of Sonde1.  The output of the Sonde1TEMP component would then be explicitly linked to the input of the forecast component.  This is a rather long-winded and complicated way of grouping the components together.

For the second point, the depth parameter is of interest to users as the position of the phenomena being forecast is important contextual information.  If a separate instance of the forecast process is created for every sensor being forecast, then it would seem reasonable to store the depth and other associated parameters within the instance definition.  This could be done as a modifiable setting parameter or mode setting from the base instance.

The final observation layout for forecast-generated observations is as below:

![Sensor Observation Model](graphics/ForecastObservation.png?raw=true "Forecast Observation")

### QC Observation Data

QC observation data refers to the output of QC routines that are used to attempt identification of observations that require closer evaluation by a data manager.  The questions raised above regarding forecast representation are similar to those raised with QC processes, though they appear at first simplified, as the QC observations are not represented as separate observation streams, and are instead included as meta-data with the observations they relate to.  Therefore it is enough to include them as inline components, inherited from a base process using typeOf, as modelled below.

![Sensor Observation Model](graphics/QC_Unit_Design.png?raw=true "QC Unit")

This represents an aggregate process called 'QC Unit', with multiple simple processes.  The simple processes provide the individual types of check, such as using a static forecast pre-compiled each year, the recursive TEDA implementation, or dynamic harmonic regression.  All these examples are state-full, in that the bounds or model parameters are relative to what is being checked.  For such variables it is enough to define the configurable properties in the original definitions, and to set the values for the specific case when implemented.  The aggregate process 'QC Unit' is only here for example purposes, in the system there will be multiple different units, with a mixed bag of QC checks grouped together for different types of phenomena/sensor.  For example, sensors that record phenomena with a strong diurnal and seasonal signal may have a different group of checks than those recording biological or more varied signals.

The below observation diagram shows that the feature of interest for the QC checks is the observation itself, similar to that of a specimen, however as the QC output is saved as meta-data of the observation, this modelling approach is not necessary and is only included here to demonstrate the relationship between the QC checks, the observation and the feature.

![Sensor Observation Model](graphics/QCObservationReferencingSensor.png?raw=true "QC Observation Sensor")

# Datastore Design

For simplicity the initial design took the form of the basic OM_Measurement schema rather than the time-series schema.  This basic design is then amended where necessary to support features of the time-series schema.

## Design Choices

The overarching design choice of this system is to store as little data as possible within the database.  Only the information regarding observations that cannot be moved or that do not make sense to move to the catalogue remains here.  This means that the only information stored in the database relates directly to the individual observations, everything else will link directly back to the catalogue reference (e.g. the procedure ID, feature ID, measurement unit ID).

After this there are lesser design choices such as:

* store the bulk of the QC data separately from the main observation data, keeping only the overall quality and accuracy fields with the observation data.  For a large number of use-cases only the aggregate QC flags are necessary, so it makes sense to keep the bulk QC data in a separate table which is only retrieved when explicitly requested.

* assume that the higher level layers can collate temporal data together.  The catalogue will store the temporal extent of any observation series, however a single row in the database will only hold a single temporal group (e.g. a month, quarter, year) of the series, so the calling software will need to pull the different rows together.  A similar task will need to be carried out to collate full QC data to observation data.

* assume that the higher level layer will collate the output of different procedures, features, phenomena, and observation offerings where necessary.  Each row in the database will correspond to a unique feature, property, process, temporal period combination, and so for example service calls requesting multiple processes or features, the higher level layer will need to request each unique row combination and collate the results.

## Catalogue-Held Data

Examples of information to be held in the catalogue relating to the point observations and time-series are shown in the list below.  For many of these entries, they are marked as "not expected to change often" - while they may change in the future, these changes are predicted to be so few that it makes more sense to track the versions within the catalogue than in the database.

* intendedObservationSpacing
    + the expected gap between point observations, held at the procedure level as not expected to change often.
* sampledMedium
    + the medium in which the observed property or phenomena is observed, held at the procedure level as not expected to change often.
* baseTime and spacing
    + domain representation orientated.  Covered somewhat by temporal extent and intentedObservationSpacing
* interpolationCode
    + the interpolation type, for example continuous/instantaneous for high-resolution sensors, discontinuous for manual sampling, average in preceding/succeeding interval for hourly and daily averages.  Not expected to change over time for a series.
* aggregationDuration
    + for aggregate interpolation types, not expected to change over time
* start/endAnchorPoint
    + for aggregate interpolation types, to define the start and end points of a series.  For example the first average in preceding interval hourly aggregate would have a start anchor point to denote where the first aggregate point's inclusive data covers.
* cumulative, accumulationIntervalLength, accumulationAnchorTime
    + for series such as total rainfall in a given period, not expected to change over time
* temporalExtent
    + The range of time the series represents.  Likely to be updated as observations are added to the system.

## Database-Held Data

Meta-data to include within the database potentially for every recorded observation:

* quality
    + an overall qualitative flag representing the quality of the observation
* accuracy
    + a quantitative measure representing the quality of the observation
* status
    + the current status of the observation, such as provisional, processed, validated, published
* processing 
    + allowing categorisation of the processing that has been applied, separately from the process information.  For example, the process may be the physical sensor, but, this may refer to differing levels of QC processing (used in conjunction with status).  It could also indicate the QC Unit used to process the observation, or the manual sampling regime handbook followed by the human used as the process

Meta-data to store in separate tables from the observations, to join when necessary:

* nilReason and/or censoredReason
    + held separately as few observations are censored (for example, when over 50% of observations in a hourly or daily summary are deemed low quality) or set to nil (NA) value, and so a table will be used to hold the sporadic entries

* qualifier and comment
    + qualifier relates to zero or more quality entries per-observation that give more detail to the overall qualitative/quantitative quality flag.  The respective output of multiple QC tests would be held together for each observation.  Held apart from the observation table as not all use-cases will need this data, and so it shortens the read times when accessing the observation data to not read information unnecessarily.  The comment is held alongside the qualifier inputs as it is expected that the QC procedures will reach a point where they annotate the output to help data managers better understand their outcomes.

## Table Design

* While the column phenomenontimeend is stored for observation data, it is not stored for QC data.  This is due to it being unnecessary for merging data as an unique set of procedure/feature/observableproperty values will generate an observation series with no overlapping temporal points.


### Observation Table

Fields within the table are:

* procedure: a URI to the catalogue
* feature: a URI to the catalogue
* observableproperty: a URI to the vocabulary
* year: the year the observations relate to 
* month: the month within the year the observations relate to
* phenomenontimestart: the start of the phenomenon time (or the time point when using TVP representation)
* phenomenontimeend: kept though not necessary for TVP representation
* value: the observed value
* quality: the overall quality flag for the observation value
* accuracy: the overall quality numeric indicating the believed accuracy of the value
* status: the status of the observations, whether processed, validated, provisional, published etc.
* processing: the type of QC unit applied to the observation
* uncertml: a text entry to store JSON representation of UncertML entries
* comment: a text entry for free description of annotation of the qualifier output

The row key is:

* procedure, feature, observableproperty, year, month

The column key is:

* phenomenontimestart

### Observation Forecast Table

This is the same as the Observation Table above, except that there are now four extra fields:

* resulttimestart
* resulttimeend
* validtimestart
* validtimeend

Of which resulttimestart and validtimestart are part of the column key with phenomenontimestart.

### Observation QC Table

The observation QC table holds the qualifier entries and comment associated with an observation.  As there can be multiple qualifiers, and the amount can change over time as techniques are added or removed depending on performance, the qualifier URI will be part of the column key.

* procedure: a URI to the catalogue
* feature: a URI to the catalogue
* observableproperty: a URI to the vocabulary
* year: the year the observations relate to 
* month: the month within the year the observations relate to
* phenomenontimestart: the start of the phenomenon time (or the time point when using TVP representation)
* qualifier: a URI to the catalogue defining the QC check
* qualifiervalue: the outcome value associated with the qualifier used
* comment: a text entry for free description or annotation of the qualifier output

The row key is:

* procedure, feature, observableproperty, year, month

The column key is:

* phenomnontimestart, qualifier

### Observation Nil/Censored Tables

Originally it was thought to remove the observation nil, censored reasons out of the observation table.  This view changed however to one where this scenario can be represented using the correct overall flag and observation value comment.  If a value is nil or censored, the overall flag will represent this, and the reason for this flag will be stored within the comment column.  It will be up to the higher level software to censure the observation value when/where necessary.